{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1eccdfe",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Populate the environment variables or edit the variables below before running:\n",
    "- `MIDDLEWARE_URL` (e.g., http://localhost:8000)\n",
    "- `MIDDLEWARE_API_KEY` (local.api_key from config.yaml)\n",
    "- `MIDDLEWARE_API_VERSION` (defaults to 2024-02-01)\n",
    "- `CHAT_MODEL` (e.g., gpt-4.1-nano)\n",
    "- `THINKING_MODEL` (e.g., gpt-5-nano)\n",
    "- `EMBEDDING_MODEL` (e.g., text-embedding-3-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617f0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "from typing import Any\n",
    "\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "API_KEY = \"sk-12345678\"\n",
    "API_VERSION = \"2024-02-01\"\n",
    "DEPLOYMENT_CHAT = \"gpt-4.1-nano\"\n",
    "DEPLOYMENT_THINKING = \"gpt-5-nano\"\n",
    "DEPLOYMENT_EMBEDDING = \"text-embedding-3-small\"\n",
    "\n",
    "HEADERS = {\"api-key\": API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def build_url(path: str) -> str:\n",
    "    return f\"{BASE_URL.rstrip('/')}{path}\"\n",
    "\n",
    "def post_json(path: str, payload: dict[str, Any], *, stream: bool = False) -> requests.Response:\n",
    "    response = requests.post(\n",
    "        build_url(path),\n",
    "        params={\"api-version\": API_VERSION},\n",
    "        headers=HEADERS,\n",
    "        json=payload,\n",
    "        stream=stream,\n",
    "        timeout=120,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response\n",
    "\n",
    "def print_json(data: Any) -> None:\n",
    "    print(json.dumps(data, indent=2))\n",
    "\n",
    "def iter_sse(resp: requests.Response):\n",
    "    for line in resp.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(b\"data: \"):\n",
    "            yield line.replace(b\"data: \", b\"\", 1).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9fb9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health: {'status': 'healthy', 'timestamp': '2025-12-14T14:55:25.634504+00:00'}\n",
      "Metrics: {'daily_cost_eur': 0.2469, 'daily_cap_eur': 5.0, 'date': '2025-12-14', 'percentage_used': 4.94}\n"
     ]
    }
   ],
   "source": [
    "# Health and metrics\n",
    "print('Health:', requests.get(build_url('/health'), timeout=10).json())\n",
    "print('Metrics:', requests.get(build_url('/metrics'), timeout=10).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e000f1d",
   "metadata": {},
   "source": [
    "## Chat completions: basic request\n",
    "Minimal call using gpt-4.1-nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"protected_material_code\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"protected_material_text\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"annotations\": [],\n",
      "        \"content\": \"Mars, Jupiter, Saturn\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1765724137,\n",
      "  \"id\": \"chatcmpl-CmhnVzXVVTlqGiMOqjDgJUw6rQynz\",\n",
      "  \"model\": \"gpt-4.1-nano-2025-04-14\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"system_fingerprint\": \"fp_03e44fcc34\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 6,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens\": 21,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 27\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chat_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"List three planets.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 100,\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_CHAT}/chat/completions\", chat_payload)\n",
    "print_json(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4778d79",
   "metadata": {},
   "source": [
    "## Chat completions: parameter coverage\n",
    "Showcase of the supported knobs (temperature, top_p, stop, penalties, n, seed, user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6800712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"protected_material_code\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"protected_material_text\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"annotations\": [],\n",
      "        \"content\": \"- Use the Tokyo Metro and JR Pass for cost-effective and convenient transportation around the city.  \\n- Explore popular neighborhoods like Shibuya, Shinjuku, and Asakusa, and don't miss local attractions such as Meiji Shrine and Tsukiji Fish Market.\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1765707366,\n",
      "  \"id\": \"chatcmpl-CmdR0JkfFNsoMYU6mBX98fkmgiS6r\",\n",
      "  \"model\": \"gpt-4.1-nano-2025-04-14\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"system_fingerprint\": \"fp_03e44fcc34\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 55,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens\": 24,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 79\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "param_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Answer in two short bullets.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give quick travel tips for Tokyo.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 120,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stop\": [\"Stop\"],\n",
    "    \"presence_penalty\": 0.2,\n",
    "    \"frequency_penalty\": 0.1,\n",
    "    \"n\": 1,\n",
    "    \"seed\": 1234,\n",
    "    \"user\": \"sample-user-123\",\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_CHAT}/chat/completions\", param_payload)\n",
    "print_json(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7d930",
   "metadata": {},
   "source": [
    "## Streaming chat\n",
    "Uses `stream=True` and reads SSE lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ece77828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  \n",
      "2  \n",
      "3  \n",
      "4  \n",
      "5  \n",
      "6  \n",
      "7  \n",
      "8  \n",
      "9  \n",
      "10  \n",
      "11  \n",
      "12  \n",
      "13  \n",
      "14  \n",
      "15  \n",
      "16  \n",
      "17  \n",
      "18  \n",
      "19  \n",
      "20  \n",
      "21  \n",
      "22  \n",
      "23  \n",
      "24  \n",
      "25  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Count from 1 to 50, one number per token.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 50,\n",
    "    \"stream\": True,\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_CHAT}/chat/completions\", stream_payload, stream=True)\n",
    "for raw in iter_sse(resp):\n",
    "    if raw == '[DONE]':\n",
    "        break\n",
    "    chunk = json.loads(raw)\n",
    "    # Some chunks have empty choices (e.g., initial metadata, final usage stats)\n",
    "    choices = chunk.get('choices', [])\n",
    "    if choices:\n",
    "        delta = choices[0].get('delta', {})\n",
    "        content = delta.get('content')\n",
    "        if content:\n",
    "            print(content, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bd125",
   "metadata": {},
   "source": [
    "## Thinking model example (gpt-5-nano)\n",
    "Let the model use reasoning tokens; output may be empty if all tokens are used for reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7edea479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 42. \n",
      "Reason: 15 = 10 + 5 and 27 = 20 + 7, so (10+20) + (5+7) = 30 + 12 = 42.\n",
      "Reasoning tokens: 320\n"
     ]
    }
   ],
   "source": [
    "# Note: Thinking models do NOT support temperature, top_p, or other sampling parameters\n",
    "thinking_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is 15 + 27? Explain briefly.\"}\n",
    "    ],\n",
    "    # No temperature! Reasoning models have fixed sampling behavior\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_THINKING}/chat/completions\", thinking_payload)\n",
    "data = resp.json()\n",
    "content = data['choices'][0]['message'].get('content')\n",
    "print('Answer:', content or '(reasoning-only tokens)')\n",
    "usage = data.get('usage', {})\n",
    "details = usage.get('completion_tokens_details')\n",
    "if details:\n",
    "    print('Reasoning tokens:', details.get('reasoning_tokens'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97dca21",
   "metadata": {},
   "source": [
    "## Structured output (JSON schema)\n",
    "Requests a JSON object shaped by a schema. Note: if your middleware version strips unknown fields, add them to `ChatCompletionRequest` or allow extras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbd08a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"winner\": \"Los Angeles Dodgers\",\n",
      "  \"series\": \"2020 World Series\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Azure OpenAI config\n",
    "endpoint = \"http://localhost:8000\"  # Your Azure endpoint\n",
    "api_key = \"sk-12345678\"\n",
    "deployment_name = \"gpt-4.1-nano\"  # Azure deployment\n",
    "\n",
    "# Initialize client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=deployment_name, # Model = should match the deployment name you chose for your model deployment\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f86cc3",
   "metadata": {},
   "source": [
    "## Tool calling\n",
    "Send tool definitions; inspect tool calls in the response. Note: ensure your middleware forwards `tools` and `tool_choice` without stripping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af0a7032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_OapbiMJQkHd4d0m2wuTKliSv', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_current_time'), type='function')])\n",
      "Function arguments: {'location': 'San Francisco'}\n",
      "get_current_time called with location: San Francisco\n",
      "Timezone found for san francisco\n",
      "The current time in San Francisco is 2:16 AM.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Azure OpenAI config\n",
    "endpoint = \"http://localhost:8000\"  # Your Azure endpoint\n",
    "api_key = \"sk-12345678\"\n",
    "deployment_name = \"gpt-4.1-nano\"  # Azure deployment\n",
    "\n",
    "# Initialize client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Simplified timezone data\n",
    "TIMEZONE_DATA = {\n",
    "    \"tokyo\": \"Asia/Tokyo\",\n",
    "    \"san francisco\": \"America/Los_Angeles\",\n",
    "    \"paris\": \"Europe/Paris\"\n",
    "}\n",
    "\n",
    "def get_current_time(location):\n",
    "    \"\"\"Get the current time for a given location\"\"\"\n",
    "    print(f\"get_current_time called with location: {location}\")  \n",
    "    location_lower = location.lower()\n",
    "    \n",
    "    for key, timezone in TIMEZONE_DATA.items():\n",
    "        if key in location_lower:\n",
    "            print(f\"Timezone found for {key}\")  \n",
    "            current_time = datetime.now(ZoneInfo(timezone)).strftime(\"%I:%M %p\")\n",
    "            return json.dumps({\n",
    "                \"location\": location,\n",
    "                \"current_time\": current_time\n",
    "            })\n",
    "    \n",
    "    print(f\"No timezone data found for {location_lower}\")  \n",
    "    return json.dumps({\"location\": location, \"current_time\": \"unknown\"})\n",
    "\n",
    "def run_conversation():\n",
    "    # Initial user message\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the current time in San Francisco\"}] # Single function call\n",
    "    #messages = [{\"role\": \"user\", \"content\": \"What's the current time in San Francisco, Tokyo, and Paris?\"}] # Parallel function call with a single tool/function defined\n",
    "\n",
    "    # Define the function for the model\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_time\",\n",
    "                \"description\": \"Get the current time in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city name, e.g. San Francisco\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # First API call: Ask the model to use the function\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\"name\": \"get_current_time\"} # Forces this specific function\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Process the model's response\n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "\n",
    "    print(\"Model's response:\")  \n",
    "    print(response_message)  \n",
    "\n",
    "    # Handle function calls\n",
    "    if response_message.tool_calls:\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            if tool_call.function.name == \"get_current_time\":\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"Function arguments: {function_args}\")  \n",
    "                time_response = get_current_time(\n",
    "                    location=function_args.get(\"location\")\n",
    "                )\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": \"get_current_time\",\n",
    "                    \"content\": time_response,\n",
    "                })\n",
    "    else:\n",
    "        print(\"No tool calls were made by the model.\")  \n",
    "\n",
    "    # Second API call: Get the final response from the model\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Run the conversation and print the result\n",
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407400e",
   "metadata": {},
   "source": [
    "## Vision: sending a photo (image_url)\n",
    "Uses a tiny inline PNG. Replace with your own base64 or hosted URL. Note: make sure your deployment supports vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683597fb",
   "metadata": {},
   "source": [
    "## Vision: base64 from repo image\n",
    "Encode `examples/element/example.png` to base64 and send as `image_url`. This avoids embedding large binaries directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19f5afcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The picture shows a dog standing on four soda cans, with a hat placed on its head. The cans are labeled with humorous text: \n",
      "- The front can is labeled \"Indian guys on YouTube.\"\n",
      "- The middle can is labeled \"Stack Overflow.\"\n",
      "- The back can is labeled \"Luck.\"\n",
      "- The dog itself has the caption \"My code,\" suggesting the dog's balancing act represents the challenges or chaos of coding. \n",
      "The setting appears to be against a tiled wall on a red cloth or surface.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import base64\n",
    "from mimetypes import guess_type\n",
    "\n",
    "# Azure OpenAI config\n",
    "endpoint = \"http://localhost:8000\"  # Your Azure endpoint\n",
    "api_key = \"sk-12345678\"\n",
    "deployment_name = \"gpt-4.1-nano\"  # Azure deployment\n",
    "\n",
    "# Initialize client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Read image and encode to base64\n",
    "def local_image_to_data_url(image_path):\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = \"application/octet-stream\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"data:{mime_type};base64,{encoded}\"\n",
    "# ---------------------------\n",
    "# Image input\n",
    "# ---------------------------\n",
    "image_path = r\"P:\\Alan\\Github\\AzureMiddleware\\examples\\element\\example.png\"  # Replace with your local image path\n",
    "data_url = local_image_to_data_url(image_path)\n",
    "\n",
    "# ---------------------------\n",
    "# Make the chat completion call\n",
    "# ---------------------------\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe this picture:\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "        ]}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Print the assistant's response\n",
    "# ---------------------------\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2349fe9",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Create embeddings and inspect dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8dcaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors: 2 Dimension of first vector: 256\n"
     ]
    }
   ],
   "source": [
    "emb_payload = {\n",
    "    \"input\": [\"First text\", \"Second text\"],\n",
    "    \"dimensions\": 256,\n",
    "    \"encoding_format\": \"float\",\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_EMBEDDING}/embeddings\", emb_payload)\n",
    "data = resp.json()\n",
    "print('Vectors:', len(data['data']), 'Dimension of first vector:', len(data['data'][0]['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a853054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.019244952127337456, 0.0037762185093015432, -0.03293963521718979, 0.0037592509761452675, 0.008121\n",
      "[-0.01016364898532629, 0.02342759631574154, -0.04225384443998337, -0.0015080638695508242, -0.0235117\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    api_key=\"sk-12345678\",\n",
    "    azure_endpoint=\"http://localhost:8000\",\n",
    "    openai_api_version=\"2024-12-01-preview\",\n",
    ")\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "two_vectors = embeddings.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8152827",
   "metadata": {},
   "source": [
    "## Responses API example\n",
    "Lightweight example with instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76afa3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PROMPT: Summarize Basel III market risk framework.\n",
      "RESPONSE: The Basel III market risk framework is a set of international banking regulations developed by the Basel Committee on Banking Supervision to strengthen the resilience of banks against market risks. It builds upon the previous Basel II standards and introduces more rigorous standards for measuring, managing, and capitalizing against market risk exposures.\n",
      "\n",
      "**Key Components of the Basel III Market Risk Framework:**\n",
      "\n",
      "1. **Scope and Objectives:**\n",
      "   - Enhances the measurement and management of market risks, including interest rate risk, equity risk, foreign exchange risk, and commodity risk.\n",
      "   - Applies to all banking organizations with significant trading activities.\n",
      "\n",
      "2. **Standardized and Internal Models Approaches:**\n",
      "   - **Standardized Approach:** Provides a consistent, regulator-approved method for calculating market risk capital requirements based on predefined risk weights and sensitivities.\n",
      "   - **Internal Models Approach (IMA):** Allows banks to use their internal risk models, subject to regulatory approval, to estimate market risk capital requirements more accurately.\n",
      "\n",
      "3. **Risk Measures and Capital Requirements\n",
      "LATENCY: 5.08\n",
      "==================================================\n",
      "PROMPT: Explain Value at Risk in simple terms.\n",
      "RESPONSE: Sure! \n",
      "\n",
      "**Value at Risk (VaR)** is a way to measure how much money you could potentially lose in a certain period of time, with a certain level of confidence.\n",
      "\n",
      "**Think of it like this:**  \n",
      "Imagine you have a bag of 100 coins, and you're worried about losing some. VaR tells you, \"With 95% confidence, you won't lose more than 10 coins in a day.\" So, it gives you an estimate of the worst expected loss over a specific time frame, under normal market conditions.\n",
      "\n",
      "**In simple terms:**  \n",
      "- VaR answers the question: \"How much could I lose, and how sure am I about that?\"  \n",
      "- It helps investors and companies understand and manage their risk by quantifying potential losses.\n",
      "\n",
      "**Remember:**  \n",
      "- VaR doesn't tell you the maximum possible loss, just a level of loss that is unlikely to be exceeded.  \n",
      "- It's a useful tool for risk management but should be used alongside other measures.\n",
      "LATENCY: 5.08\n",
      "==================================================\n",
      "PROMPT: Explain stress testing in banks.\n",
      "RESPONSE: **Stress Testing in Banks**\n",
      "\n",
      "**Definition:**\n",
      "Stress testing is a risk management tool used by banks to evaluate their financial resilience under adverse economic or financial conditions. It involves simulating hypothetical or historical scenarios to assess how various stress factors could impact a bank’s capital adequacy, liquidity, and overall stability.\n",
      "\n",
      "**Purpose:**\n",
      "- To identify vulnerabilities in a bank’s portfolio and operations.\n",
      "- To ensure the bank maintains sufficient capital buffers during economic downturns.\n",
      "- To comply with regulatory requirements and enhance risk management practices.\n",
      "- To inform strategic decision-making and contingency planning.\n",
      "\n",
      "**Key Components:**\n",
      "1. **Scenario Selection:**  \n",
      "   Banks develop scenarios that may include severe economic downturns, market crashes, interest rate shocks, or geopolitical crises. These can be hypothetical or based on past crises.\n",
      "\n",
      "2. **Risk Factors Assessed:**  \n",
      "   - Credit risk (defaults on loans and investments)  \n",
      "   - Market risk (interest rate, foreign exchange, equity prices)  \n",
      "   - Liquidity risk (\n",
      "LATENCY: 5.56\n",
      "==================================================\n",
      "PROMPT: What is expected shortfall?\n",
      "RESPONSE: Expected Shortfall (ES), also known as Conditional Value at Risk (CVaR), is a risk measure used in finance and risk management to assess the potential for extreme losses in a portfolio or investment.\n",
      "\n",
      "**Definition:**\n",
      "Expected Shortfall at a given confidence level (e.g., 95%) is the average of all losses that occur beyond the Value at Risk (VaR) threshold at that confidence level. In other words, it provides the expected loss assuming that the loss has exceeded the VaR.\n",
      "\n",
      "**Intuition:**\n",
      "While VaR tells you the maximum loss not exceeded with a certain confidence (e.g., 95%), it does not specify how severe losses could be beyond that point. Expected Shortfall addresses this by averaging those worst-case losses, giving a more comprehensive picture of tail risk.\n",
      "\n",
      "**Mathematically:**\n",
      "For a loss random variable \\( L \\), the Expected Shortfall at confidence level \\( \\alpha \\) (e.g., 0.95) is:\n",
      "\n",
      "\n",
      "LATENCY: 5.56\n",
      "==================================================\n",
      "PROMPT: Difference between VaR and ES?\n",
      "RESPONSE: Value at Risk (VaR) and Expected Shortfall (ES), also known as Conditional VaR, are both risk measures used to assess the potential losses in a portfolio. However, they differ in what they quantify and how they are calculated:\n",
      "\n",
      "**1. Definition:**\n",
      "\n",
      "- **VaR (Value at Risk):**  \n",
      "  The maximum loss not exceeded with a certain confidence level over a specified time horizon. For example, a 1-day 95% VaR of $1 million means there is a 95% chance that losses will not exceed $1 million in one day.\n",
      "\n",
      "- **ES (Expected Shortfall):**  \n",
      "  The average loss given that the loss exceeds the VaR threshold. It measures the expected loss in the worst-case tail beyond the VaR level.\n",
      "\n",
      "**2. Focus:**\n",
      "\n",
      "- **VaR:**  \n",
      "  Focuses on a quantile of the loss distribution; it tells you the threshold loss at a certain confidence level but does not specify the severity\n",
      "LATENCY: 2.41\n",
      "==================================================\n",
      "PROMPT: Explain Value at Risk in simple terms.\n",
      "RESPONSE: Sure! \n",
      "\n",
      "**Value at Risk (VaR)** is a way to measure how much money you could potentially lose in a certain period of time, with a certain level of confidence.\n",
      "\n",
      "**In simple terms:**\n",
      "\n",
      "- Imagine you have some investments or a portfolio.\n",
      "- VaR tells you, \"There's a 95% chance that you won't lose more than a certain amount over the next month.\"\n",
      "- That \"certain amount\" is the VaR figure.\n",
      "\n",
      "**For example:**\n",
      "\n",
      "If your one-month VaR at 95% confidence is $10,000, it means:\n",
      "\n",
      "- There's a 95% chance you'll lose **less than** $10,000 in the next month.\n",
      "- Conversely, there's a 5% chance you'll lose **more than** $10,000.\n",
      "\n",
      "**In summary:**\n",
      "\n",
      "VaR helps you understand the worst-case loss you might face within a certain confidence level and time frame, so you can plan and manage your risks accordingly.\n",
      "LATENCY: 2.65\n",
      "==================================================\n",
      "PROMPT: Summarize Basel III market risk framework.\n",
      "RESPONSE: The Basel III Market Risk Framework is a set of international regulatory standards developed by the Basel Committee on Banking Supervision to strengthen the regulation, supervision, and risk management of market risks in banking organizations. It aims to ensure banks hold sufficient capital to cover potential losses from market fluctuations, thereby promoting financial stability.\n",
      "\n",
      "**Key Components of the Basel III Market Risk Framework:**\n",
      "\n",
      "1. **Scope and Objectives:**\n",
      "   - Extends the Basel II market risk standards with enhanced risk sensitivity.\n",
      "   - Applies to trading book exposures, including derivatives, securities, and other financial instruments.\n",
      "\n",
      "2. **Standardized and Internal Models Approaches:**\n",
      "   - **Standardized Approach (SA):** Provides a consistent, transparent method for calculating market risk capital requirements using predefined risk weights and sensitivities.\n",
      "   - **Internal Models Approach (IMA):** Allows banks to use their internal risk models, subject to supervisory approval, to calculate capital requirements based on Value-at-Risk (VaR) and stressed VaR (sVa\n",
      "LATENCY: 2.42\n",
      "==================================================\n",
      "PROMPT: What is expected shortfall?\n",
      "RESPONSE: Expected Shortfall (ES), also known as Conditional Value at Risk (CVaR), is a risk measure used in finance to assess the potential for extreme losses in a portfolio or investment. It provides an estimate of the average loss that would occur in the worst-case scenarios beyond a specified confidence level.\n",
      "\n",
      "**Key points about Expected Shortfall:**\n",
      "\n",
      "- **Definition:** For a given confidence level \\( \\alpha \\) (e.g., 95%), the Expected Shortfall is the average of all losses that occur beyond the Value at Risk (VaR) at that level. In other words, it measures the expected loss in the worst \\( (1 - \\alpha) \\times 100\\% \\) of cases.\n",
      "\n",
      "- **Mathematically:**\n",
      "  \\[\n",
      "  \\text{ES}_\\alpha = \\mathbb{E}[L \\mid L \\geq \\text{VaR}_\\alpha]\n",
      "  \\]\n",
      "  where \\( L \\) is the loss random variable\n",
      "LATENCY: 2.52\n",
      "==================================================\n",
      "PROMPT: Explain stress testing in banks.\n",
      "RESPONSE: **Stress Testing in Banks**\n",
      "\n",
      "**Definition:**\n",
      "Stress testing is a risk management tool used by banks to evaluate their financial resilience under hypothetical adverse economic or financial scenarios. It involves simulating extreme but plausible events to assess the potential impact on a bank’s capital adequacy, liquidity, and overall stability.\n",
      "\n",
      "**Purpose:**\n",
      "- To identify vulnerabilities in a bank’s portfolio and operations.\n",
      "- To ensure the bank has sufficient capital buffers to withstand economic shocks.\n",
      "- To comply with regulatory requirements and enhance risk management practices.\n",
      "- To inform strategic decision-making and contingency planning.\n",
      "\n",
      "**Key Components:**\n",
      "1. **Scenario Development:**  \n",
      "   Banks develop severe but plausible scenarios, such as a recession, sharp decline in asset prices, or a sudden increase in default rates.\n",
      "\n",
      "2. **Impact Analysis:**  \n",
      "   The bank models how these scenarios would affect its assets, liabilities, income, and capital. This includes assessing credit risk, market risk, liquidity risk, and operational risk.\n",
      "\n",
      "3. **Assessment of Capital Ade\n",
      "LATENCY: 2.31\n",
      "==================================================\n",
      "PROMPT: Difference between VaR and ES?\n",
      "RESPONSE: Value at Risk (VaR) and Expected Shortfall (ES), also known as Conditional VaR, are both risk measures used to assess the potential losses in a portfolio, but they differ in their definitions and the information they provide.\n",
      "\n",
      "**1. Value at Risk (VaR):**  \n",
      "- **Definition:** VaR at a confidence level \\( \\alpha \\) (e.g., 95%) is the maximum loss not exceeded with probability \\( \\alpha \\) over a specified time horizon.  \n",
      "- **Interpretation:** If the 95% VaR is \\$1 million, there is a 95% chance that losses will not exceed \\$1 million in the given period.  \n",
      "- **Mathematically:**  \n",
      "  \\[\n",
      "  \\text{VaR}_\\alpha = \\inf \\{ x \\in \\mathbb{R} : P(Loss > x) \\leq 1 - \\alpha \\}\n",
      "  \\]\n",
      "- **Limitations:**  \n",
      "  -\n",
      "LATENCY: 2.58\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.messages import HumanMessage\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# LangChain model initialization\n",
    "# -----------------------------\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    api_key=\"sk-12345678\",\n",
    "    azure_endpoint=\"http://localhost:8000\",\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Worker function (thread-safe)\n",
    "# -----------------------------\n",
    "def invoke_llm(prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Each thread runs this function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = llm.invoke(\n",
    "        [HumanMessage(content=prompt)]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response.content,\n",
    "        \"latency_sec\": round(time.time() - start_time, 2),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Main multithread runner\n",
    "# -----------------------------\n",
    "def run_multithreaded(prompts, max_workers=5):\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_prompt = {\n",
    "            executor.submit(invoke_llm, prompt): prompt\n",
    "            for prompt in prompts\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_prompt):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"prompt\": future_to_prompt[future],\n",
    "                    \"error\": str(e),\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    prompts = [\n",
    "        \"Explain Value at Risk in simple terms.\",\n",
    "        \"Summarize Basel III market risk framework.\",\n",
    "        \"What is expected shortfall?\",\n",
    "        \"Explain stress testing in banks.\",\n",
    "        \"Difference between VaR and ES?\",\n",
    "        \"Explain Value at Risk in simple terms.\",\n",
    "        \"Summarize Basel III market risk framework.\",\n",
    "        \"What is expected shortfall?\",\n",
    "        \"Explain stress testing in banks.\",\n",
    "        \"Difference between VaR and ES?\"\n",
    "    ]\n",
    "\n",
    "    outputs = run_multithreaded(prompts, max_workers=4)\n",
    "\n",
    "    for o in outputs:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"PROMPT:\", o.get(\"prompt\"))\n",
    "        print(\"RESPONSE:\", o.get(\"response\"))\n",
    "        print(\"LATENCY:\", o.get(\"latency_sec\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
