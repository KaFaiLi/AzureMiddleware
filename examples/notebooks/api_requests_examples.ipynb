{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1eccdfe",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Populate the environment variables or edit the variables below before running:\n",
    "- `MIDDLEWARE_URL` (e.g., http://localhost:8000)\n",
    "- `MIDDLEWARE_API_KEY` (local.api_key from config.yaml)\n",
    "- `MIDDLEWARE_API_VERSION` (defaults to 2024-02-01)\n",
    "- `CHAT_MODEL` (e.g., gpt-4.1-nano)\n",
    "- `THINKING_MODEL` (e.g., gpt-5-nano)\n",
    "- `EMBEDDING_MODEL` (e.g., text-embedding-3-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617f0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "from typing import Any\n",
    "\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "API_KEY = \"sk-12345678\"\n",
    "API_VERSION = \"2024-02-01\"\n",
    "DEPLOYMENT_CHAT = \"gpt-4.1-nano\"\n",
    "DEPLOYMENT_THINKING = \"gpt-5-nano\"\n",
    "DEPLOYMENT_EMBEDDING = \"text-embedding-3-small\"\n",
    "\n",
    "HEADERS = {\"api-key\": API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def build_url(path: str) -> str:\n",
    "    return f\"{BASE_URL.rstrip('/')}{path}\"\n",
    "\n",
    "def post_json(path: str, payload: dict[str, Any], *, stream: bool = False) -> requests.Response:\n",
    "    response = requests.post(\n",
    "        build_url(path),\n",
    "        params={\"api-version\": API_VERSION},\n",
    "        headers=HEADERS,\n",
    "        json=payload,\n",
    "        stream=stream,\n",
    "        timeout=120,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response\n",
    "\n",
    "def print_json(data: Any) -> None:\n",
    "    print(json.dumps(data, indent=2))\n",
    "\n",
    "def iter_sse(resp: requests.Response):\n",
    "    for line in resp.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(b\"data: \"):\n",
    "            yield line.replace(b\"data: \", b\"\", 1).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9fb9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health: {'status': 'healthy', 'timestamp': '2025-12-14T14:55:25.634504+00:00'}\n",
      "Metrics: {'daily_cost_eur': 0.2469, 'daily_cap_eur': 5.0, 'date': '2025-12-14', 'percentage_used': 4.94}\n"
     ]
    }
   ],
   "source": [
    "# Health and metrics\n",
    "print('Health:', requests.get(build_url('/health'), timeout=10).json())\n",
    "print('Metrics:', requests.get(build_url('/metrics'), timeout=10).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e000f1d",
   "metadata": {},
   "source": [
    "## Chat completions: basic request\n",
    "Minimal call using gpt-4.1-nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edc6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"protected_material_code\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"protected_material_text\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"annotations\": [],\n",
      "        \"content\": \"Mars, Jupiter, Saturn\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1765724137,\n",
      "  \"id\": \"chatcmpl-CmhnVzXVVTlqGiMOqjDgJUw6rQynz\",\n",
      "  \"model\": \"gpt-4.1-nano-2025-04-14\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"system_fingerprint\": \"fp_03e44fcc34\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 6,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens\": 21,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 27\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chat_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"List three planets.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 100,\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_CHAT}/chat/completions\", chat_payload)\n",
    "print_json(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4778d79",
   "metadata": {},
   "source": [
    "## Chat completions: parameter coverage\n",
    "Showcase of the supported knobs (temperature, top_p, stop, penalties, n, seed, user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6800712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"protected_material_code\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"protected_material_text\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"annotations\": [],\n",
      "        \"content\": \"- Use the Tokyo Metro and JR Pass for cost-effective and convenient transportation around the city.  \\n- Explore popular neighborhoods like Shibuya, Shinjuku, and Asakusa, and don't miss local attractions such as Meiji Shrine and Tsukiji Fish Market.\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1765707366,\n",
      "  \"id\": \"chatcmpl-CmdR0JkfFNsoMYU6mBX98fkmgiS6r\",\n",
      "  \"model\": \"gpt-4.1-nano-2025-04-14\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"system_fingerprint\": \"fp_03e44fcc34\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 55,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens\": 24,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 79\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "param_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Answer in two short bullets.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give quick travel tips for Tokyo.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 120,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stop\": [\"Stop\"],\n",
    "    \"presence_penalty\": 0.2,\n",
    "    \"frequency_penalty\": 0.1,\n",
    "    \"n\": 1,\n",
    "    \"seed\": 1234,\n",
    "    \"user\": \"sample-user-123\",\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_CHAT}/chat/completions\", param_payload)\n",
    "print_json(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7d930",
   "metadata": {},
   "source": [
    "## Streaming chat\n",
    "Uses `stream=True` and reads SSE lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ece77828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  \n",
      "2  \n",
      "3  \n",
      "4  \n",
      "5  \n",
      "6  \n",
      "7  \n",
      "8  \n",
      "9  \n",
      "10  \n",
      "11  \n",
      "12  \n",
      "13  \n",
      "14  \n",
      "15  \n",
      "16  \n",
      "17  \n",
      "18  \n",
      "19  \n",
      "20  \n",
      "21  \n",
      "22  \n",
      "23  \n",
      "24  \n",
      "25  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Count from 1 to 50, one number per token.\"}\n",
    "    ],\n",
    "    \"max_completion_tokens\": 50,\n",
    "    \"stream\": True,\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_CHAT}/chat/completions\", stream_payload, stream=True)\n",
    "for raw in iter_sse(resp):\n",
    "    if raw == '[DONE]':\n",
    "        break\n",
    "    chunk = json.loads(raw)\n",
    "    # Some chunks have empty choices (e.g., initial metadata, final usage stats)\n",
    "    choices = chunk.get('choices', [])\n",
    "    if choices:\n",
    "        delta = choices[0].get('delta', {})\n",
    "        content = delta.get('content')\n",
    "        if content:\n",
    "            print(content, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bd125",
   "metadata": {},
   "source": [
    "## Thinking model example (gpt-5-nano)\n",
    "Let the model use reasoning tokens; output may be empty if all tokens are used for reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7edea479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 42. \n",
      "Reason: 15 = 10 + 5 and 27 = 20 + 7, so (10+20) + (5+7) = 30 + 12 = 42.\n",
      "Reasoning tokens: 320\n"
     ]
    }
   ],
   "source": [
    "# Note: Thinking models do NOT support temperature, top_p, or other sampling parameters\n",
    "thinking_payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is 15 + 27? Explain briefly.\"}\n",
    "    ],\n",
    "    # No temperature! Reasoning models have fixed sampling behavior\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_THINKING}/chat/completions\", thinking_payload)\n",
    "data = resp.json()\n",
    "content = data['choices'][0]['message'].get('content')\n",
    "print('Answer:', content or '(reasoning-only tokens)')\n",
    "usage = data.get('usage', {})\n",
    "details = usage.get('completion_tokens_details')\n",
    "if details:\n",
    "    print('Reasoning tokens:', details.get('reasoning_tokens'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97dca21",
   "metadata": {},
   "source": [
    "## Structured output (JSON schema)\n",
    "Requests a JSON object shaped by a schema. Note: if your middleware version strips unknown fields, add them to `ChatCompletionRequest` or allow extras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbd08a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"winner\": \"Los Angeles Dodgers\",\n",
      "  \"series\": \"2020 World Series\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Azure OpenAI config\n",
    "endpoint = \"http://localhost:8000\"  # Your Azure endpoint\n",
    "api_key = \"sk-12345678\"\n",
    "deployment_name = \"gpt-4.1-nano\"  # Azure deployment\n",
    "\n",
    "# Initialize client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=deployment_name, # Model = should match the deployment name you chose for your model deployment\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f86cc3",
   "metadata": {},
   "source": [
    "## Tool calling\n",
    "Send tool definitions; inspect tool calls in the response. Note: ensure your middleware forwards `tools` and `tool_choice` without stripping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af0a7032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_OapbiMJQkHd4d0m2wuTKliSv', function=Function(arguments='{\"location\":\"San Francisco\"}', name='get_current_time'), type='function')])\n",
      "Function arguments: {'location': 'San Francisco'}\n",
      "get_current_time called with location: San Francisco\n",
      "Timezone found for san francisco\n",
      "The current time in San Francisco is 2:16 AM.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Azure OpenAI config\n",
    "endpoint = \"http://localhost:8000\"  # Your Azure endpoint\n",
    "api_key = \"sk-12345678\"\n",
    "deployment_name = \"gpt-4.1-nano\"  # Azure deployment\n",
    "\n",
    "# Initialize client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Simplified timezone data\n",
    "TIMEZONE_DATA = {\n",
    "    \"tokyo\": \"Asia/Tokyo\",\n",
    "    \"san francisco\": \"America/Los_Angeles\",\n",
    "    \"paris\": \"Europe/Paris\"\n",
    "}\n",
    "\n",
    "def get_current_time(location):\n",
    "    \"\"\"Get the current time for a given location\"\"\"\n",
    "    print(f\"get_current_time called with location: {location}\")  \n",
    "    location_lower = location.lower()\n",
    "    \n",
    "    for key, timezone in TIMEZONE_DATA.items():\n",
    "        if key in location_lower:\n",
    "            print(f\"Timezone found for {key}\")  \n",
    "            current_time = datetime.now(ZoneInfo(timezone)).strftime(\"%I:%M %p\")\n",
    "            return json.dumps({\n",
    "                \"location\": location,\n",
    "                \"current_time\": current_time\n",
    "            })\n",
    "    \n",
    "    print(f\"No timezone data found for {location_lower}\")  \n",
    "    return json.dumps({\"location\": location, \"current_time\": \"unknown\"})\n",
    "\n",
    "def run_conversation():\n",
    "    # Initial user message\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the current time in San Francisco\"}] # Single function call\n",
    "    #messages = [{\"role\": \"user\", \"content\": \"What's the current time in San Francisco, Tokyo, and Paris?\"}] # Parallel function call with a single tool/function defined\n",
    "\n",
    "    # Define the function for the model\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_time\",\n",
    "                \"description\": \"Get the current time in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city name, e.g. San Francisco\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # First API call: Ask the model to use the function\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\"name\": \"get_current_time\"} # Forces this specific function\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Process the model's response\n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "\n",
    "    print(\"Model's response:\")  \n",
    "    print(response_message)  \n",
    "\n",
    "    # Handle function calls\n",
    "    if response_message.tool_calls:\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            if tool_call.function.name == \"get_current_time\":\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                print(f\"Function arguments: {function_args}\")  \n",
    "                time_response = get_current_time(\n",
    "                    location=function_args.get(\"location\")\n",
    "                )\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": \"get_current_time\",\n",
    "                    \"content\": time_response,\n",
    "                })\n",
    "    else:\n",
    "        print(\"No tool calls were made by the model.\")  \n",
    "\n",
    "    # Second API call: Get the final response from the model\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Run the conversation and print the result\n",
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407400e",
   "metadata": {},
   "source": [
    "## Vision: sending a photo (image_url)\n",
    "Uses a tiny inline PNG. Replace with your own base64 or hosted URL. Note: make sure your deployment supports vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683597fb",
   "metadata": {},
   "source": [
    "## Vision: base64 from repo image\n",
    "Encode `examples/element/example.png` to base64 and send as `image_url`. This avoids embedding large binaries directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19f5afcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The picture shows a dog standing on four soda cans, with a hat placed on its head. The cans are labeled with humorous text: \n",
      "- The front can is labeled \"Indian guys on YouTube.\"\n",
      "- The middle can is labeled \"Stack Overflow.\"\n",
      "- The back can is labeled \"Luck.\"\n",
      "- The dog itself has the caption \"My code,\" suggesting the dog's balancing act represents the challenges or chaos of coding. \n",
      "The setting appears to be against a tiled wall on a red cloth or surface.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import base64\n",
    "from mimetypes import guess_type\n",
    "\n",
    "# Azure OpenAI config\n",
    "endpoint = \"http://localhost:8000\"  # Your Azure endpoint\n",
    "api_key = \"sk-12345678\"\n",
    "deployment_name = \"gpt-4.1-nano\"  # Azure deployment\n",
    "\n",
    "# Initialize client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Read image and encode to base64\n",
    "def local_image_to_data_url(image_path):\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = \"application/octet-stream\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"data:{mime_type};base64,{encoded}\"\n",
    "# ---------------------------\n",
    "# Image input\n",
    "# ---------------------------\n",
    "image_path = r\"P:\\Alan\\Github\\AzureMiddleware\\examples\\element\\example.png\"  # Replace with your local image path\n",
    "data_url = local_image_to_data_url(image_path)\n",
    "\n",
    "# ---------------------------\n",
    "# Make the chat completion call\n",
    "# ---------------------------\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe this picture:\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "        ]}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Print the assistant's response\n",
    "# ---------------------------\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2349fe9",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Create embeddings and inspect dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8dcaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors: 2 Dimension of first vector: 256\n"
     ]
    }
   ],
   "source": [
    "emb_payload = {\n",
    "    \"input\": [\"First text\", \"Second text\"],\n",
    "    \"dimensions\": 256,\n",
    "    \"encoding_format\": \"float\",\n",
    "}\n",
    "resp = post_json(f\"/openai/deployments/{DEPLOYMENT_EMBEDDING}/embeddings\", emb_payload)\n",
    "data = resp.json()\n",
    "print('Vectors:', len(data['data']), 'Dimension of first vector:', len(data['data'][0]['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a853054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.019244952127337456, 0.0037762185093015432, -0.03293963521718979, 0.0037592509761452675, 0.008121\n",
      "[-0.01016364898532629, 0.02342759631574154, -0.04225384443998337, -0.0015080638695508242, -0.0235117\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    api_key=\"sk-12345678\",\n",
    "    azure_endpoint=\"http://localhost:8000\",\n",
    "    openai_api_version=\"2024-12-01-preview\",\n",
    ")\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "two_vectors = embeddings.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8152827",
   "metadata": {},
   "source": [
    "## Responses API example\n",
    "Lightweight example with instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76afa3c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[0;32m      5\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-12345678\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8000\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m   \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4.1-nano\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Replace with your model deployment name \u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis is a test.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mmodel_dump_json(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\resources\\responses\\responses.py:866\u001b[0m, in \u001b[0;36mResponses.create\u001b[1;34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[0;32m    865\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response \u001b[38;5;241m|\u001b[39m Stream[ResponseStreamEvent]:\n\u001b[1;32m--> 866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/responses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstructions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_output_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious_response_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_retention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Internal Server Error"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-12345678\",\n",
    "    base_url=\"http://localhost:8000\",\n",
    ")\n",
    "\n",
    "response = client.responses.create(   \n",
    "  model=\"gpt-4.1-nano\", # Replace with your model deployment name \n",
    "  input=\"This is a test.\",\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
